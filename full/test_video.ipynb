{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 1/271\n",
      "Processed frame 2/271\n",
      "Processed frame 3/271\n",
      "Processed frame 4/271\n",
      "Processed frame 5/271\n",
      "Processed frame 6/271\n",
      "Processed frame 7/271\n",
      "Processed frame 8/271\n",
      "Processed frame 9/271\n",
      "Processed frame 10/271\n",
      "Processed frame 11/271\n",
      "Processed frame 12/271\n",
      "Processed frame 13/271\n",
      "Processed frame 14/271\n",
      "Processed frame 15/271\n",
      "Processed frame 16/271\n",
      "Processed frame 17/271\n",
      "Processed frame 18/271\n",
      "Processed frame 19/271\n",
      "Processed frame 20/271\n",
      "Processed frame 21/271\n",
      "Processed frame 22/271\n",
      "Processed frame 23/271\n",
      "Processed frame 24/271\n",
      "Processed frame 25/271\n",
      "Processed frame 26/271\n",
      "Processed frame 27/271\n",
      "Processed frame 28/271\n",
      "Processed frame 29/271\n",
      "Processed frame 30/271\n",
      "Processed frame 31/271\n",
      "Processed frame 32/271\n",
      "Processed frame 33/271\n",
      "Processed frame 34/271\n",
      "Processed frame 35/271\n",
      "Processed frame 36/271\n",
      "Processed frame 37/271\n",
      "Processed frame 38/271\n",
      "Processed frame 39/271\n",
      "Processed frame 40/271\n",
      "Processed frame 41/271\n",
      "Processed frame 42/271\n",
      "Processed frame 43/271\n",
      "Processed frame 44/271\n",
      "Processed frame 45/271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     output_density_map \u001b[38;5;241m=\u001b[39m md1(input_image)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Convert the output density map tensor to numpy array\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m density_map_numpy \u001b[38;5;241m=\u001b[39m \u001b[43moutput_density_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Resize density map to match frame size\u001b[39;00m\n\u001b[0;32m     57\u001b[0m density_map_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(density_map_numpy, (frame_width, frame_height))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the md1\n",
    "md1_path = r'D:\\Jatin\\College stuff\\Major Project\\Crowd\\crowd-density-aspp\\aspp_1_800.pt'\n",
    "md1 = torch.load(md1_path, map_location=device).to(device)\n",
    "md1.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Read video file\n",
    "video_path = r'D:\\Jatin\\College stuff\\Major Project\\Crowd\\crowd-density-aspp\\263C044_060_c.mov'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Create VideoWriter object to write overlayed video\n",
    "output_video_path = 'output_video_with_overlay.avi'\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_width, frame_height))\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to PIL Image\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Apply transformation\n",
    "    input_image = transform(frame_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output_density_map = md1(input_image)\n",
    "\n",
    "    # Convert the output density map tensor to numpy array\n",
    "    density_map_numpy = output_density_map.squeeze().cpu().numpy()\n",
    "\n",
    "    # Resize density map to match frame size\n",
    "    density_map_resized = cv2.resize(density_map_numpy, (frame_width, frame_height))\n",
    "\n",
    "    # Normalize density map for visualization\n",
    "    density_map_resized = cv2.normalize(density_map_resized, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    # Overlay density map on frame\n",
    "    alpha = 0.5\n",
    "    overlay = cv2.applyColorMap(density_map_resized, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "    # Write the frame with overlay to output video\n",
    "    out.write(overlay)\n",
    "\n",
    "    # Display progress\n",
    "    frame_count += 1\n",
    "    print(f'Processed frame {frame_count}/{total_frames}')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f'Overlayed video saved at: {output_video_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 1/271\n",
      "Processed frame 2/271\n",
      "Processed frame 3/271\n",
      "Processed frame 4/271\n",
      "Processed frame 5/271\n",
      "Processed frame 6/271\n",
      "Processed frame 7/271\n",
      "Processed frame 8/271\n",
      "Processed frame 9/271\n",
      "Processed frame 10/271\n",
      "Processed frame 11/271\n",
      "Processed frame 12/271\n",
      "Processed frame 13/271\n",
      "Processed frame 14/271\n",
      "Processed frame 15/271\n",
      "Processed frame 16/271\n",
      "Processed frame 17/271\n",
      "Processed frame 18/271\n",
      "Processed frame 19/271\n",
      "Processed frame 20/271\n",
      "Processed frame 21/271\n",
      "Processed frame 22/271\n",
      "Processed frame 23/271\n",
      "Processed frame 24/271\n",
      "Processed frame 25/271\n",
      "Processed frame 26/271\n",
      "Processed frame 27/271\n",
      "Processed frame 28/271\n",
      "Processed frame 29/271\n",
      "Processed frame 30/271\n",
      "Processed frame 31/271\n",
      "Processed frame 32/271\n",
      "Processed frame 33/271\n",
      "Processed frame 34/271\n",
      "Processed frame 35/271\n",
      "Processed frame 36/271\n",
      "Processed frame 37/271\n",
      "Processed frame 38/271\n",
      "Processed frame 39/271\n",
      "Processed frame 40/271\n",
      "Processed frame 41/271\n",
      "Processed frame 42/271\n",
      "Processed frame 43/271\n",
      "Processed frame 44/271\n",
      "Processed frame 45/271\n",
      "Processed frame 46/271\n",
      "Processed frame 47/271\n",
      "Processed frame 48/271\n",
      "Processed frame 49/271\n",
      "Processed frame 50/271\n",
      "Processed frame 51/271\n",
      "Processed frame 52/271\n",
      "Processed frame 53/271\n",
      "Processed frame 54/271\n",
      "Processed frame 55/271\n",
      "Processed frame 56/271\n",
      "Processed frame 57/271\n",
      "Processed frame 58/271\n",
      "Processed frame 59/271\n",
      "Processed frame 60/271\n",
      "Processed frame 61/271\n",
      "Processed frame 62/271\n",
      "Processed frame 63/271\n",
      "Processed frame 64/271\n",
      "Processed frame 65/271\n",
      "Processed frame 66/271\n",
      "Processed frame 67/271\n",
      "Processed frame 68/271\n",
      "Processed frame 69/271\n",
      "Processed frame 70/271\n",
      "Processed frame 71/271\n",
      "Processed frame 72/271\n",
      "Processed frame 73/271\n",
      "Processed frame 74/271\n",
      "Processed frame 75/271\n",
      "Processed frame 76/271\n",
      "Processed frame 77/271\n",
      "Processed frame 78/271\n",
      "Processed frame 79/271\n",
      "Processed frame 80/271\n",
      "Processed frame 81/271\n",
      "Processed frame 82/271\n",
      "Processed frame 83/271\n",
      "Processed frame 84/271\n",
      "Processed frame 85/271\n",
      "Processed frame 86/271\n",
      "Processed frame 87/271\n",
      "Processed frame 88/271\n",
      "Processed frame 89/271\n",
      "Processed frame 90/271\n",
      "Processed frame 91/271\n",
      "Processed frame 92/271\n",
      "Processed frame 93/271\n",
      "Processed frame 94/271\n",
      "Processed frame 95/271\n",
      "Processed frame 96/271\n",
      "Processed frame 97/271\n",
      "Processed frame 98/271\n",
      "Processed frame 99/271\n",
      "Processed frame 100/271\n",
      "Processed frame 101/271\n",
      "Processed frame 102/271\n",
      "Processed frame 103/271\n",
      "Processed frame 104/271\n",
      "Processed frame 105/271\n",
      "Processed frame 106/271\n",
      "Processed frame 107/271\n",
      "Processed frame 108/271\n",
      "Processed frame 109/271\n",
      "Processed frame 110/271\n",
      "Processed frame 111/271\n",
      "Processed frame 112/271\n",
      "Processed frame 113/271\n",
      "Processed frame 114/271\n",
      "Processed frame 115/271\n",
      "Processed frame 116/271\n",
      "Processed frame 117/271\n",
      "Processed frame 118/271\n",
      "Processed frame 119/271\n",
      "Processed frame 120/271\n",
      "Processed frame 121/271\n",
      "Processed frame 122/271\n",
      "Processed frame 123/271\n",
      "Processed frame 124/271\n",
      "Processed frame 125/271\n",
      "Processed frame 126/271\n",
      "Processed frame 127/271\n",
      "Processed frame 128/271\n",
      "Processed frame 129/271\n",
      "Processed frame 130/271\n",
      "Processed frame 131/271\n",
      "Processed frame 132/271\n",
      "Processed frame 133/271\n",
      "Processed frame 134/271\n",
      "Processed frame 135/271\n",
      "Processed frame 136/271\n",
      "Processed frame 137/271\n",
      "Processed frame 138/271\n",
      "Processed frame 139/271\n",
      "Processed frame 140/271\n",
      "Processed frame 141/271\n",
      "Processed frame 142/271\n",
      "Processed frame 143/271\n",
      "Processed frame 144/271\n",
      "Processed frame 145/271\n",
      "Processed frame 146/271\n",
      "Processed frame 147/271\n",
      "Processed frame 148/271\n",
      "Processed frame 149/271\n",
      "Processed frame 150/271\n",
      "Processed frame 151/271\n",
      "Processed frame 152/271\n",
      "Processed frame 153/271\n",
      "Processed frame 154/271\n",
      "Processed frame 155/271\n",
      "Processed frame 156/271\n",
      "Processed frame 157/271\n",
      "Processed frame 158/271\n",
      "Processed frame 159/271\n",
      "Processed frame 160/271\n",
      "Processed frame 161/271\n",
      "Processed frame 162/271\n",
      "Processed frame 163/271\n",
      "Processed frame 164/271\n",
      "Processed frame 165/271\n",
      "Processed frame 166/271\n",
      "Processed frame 167/271\n",
      "Processed frame 168/271\n",
      "Processed frame 169/271\n",
      "Processed frame 170/271\n",
      "Processed frame 171/271\n",
      "Processed frame 172/271\n",
      "Processed frame 173/271\n",
      "Processed frame 174/271\n",
      "Processed frame 175/271\n",
      "Processed frame 176/271\n",
      "Processed frame 177/271\n",
      "Processed frame 178/271\n",
      "Processed frame 179/271\n",
      "Processed frame 180/271\n",
      "Processed frame 181/271\n",
      "Processed frame 182/271\n",
      "Processed frame 183/271\n",
      "Processed frame 184/271\n",
      "Processed frame 185/271\n",
      "Processed frame 186/271\n",
      "Processed frame 187/271\n",
      "Processed frame 188/271\n",
      "Processed frame 189/271\n",
      "Processed frame 190/271\n",
      "Processed frame 191/271\n",
      "Processed frame 192/271\n",
      "Processed frame 193/271\n",
      "Processed frame 194/271\n",
      "Processed frame 195/271\n",
      "Processed frame 196/271\n",
      "Processed frame 197/271\n",
      "Processed frame 198/271\n",
      "Processed frame 199/271\n",
      "Processed frame 200/271\n",
      "Processed frame 201/271\n",
      "Processed frame 202/271\n",
      "Processed frame 203/271\n",
      "Processed frame 204/271\n",
      "Processed frame 205/271\n",
      "Processed frame 206/271\n",
      "Processed frame 207/271\n",
      "Processed frame 208/271\n",
      "Processed frame 209/271\n",
      "Processed frame 210/271\n",
      "Processed frame 211/271\n",
      "Processed frame 212/271\n",
      "Processed frame 213/271\n",
      "Processed frame 214/271\n",
      "Processed frame 215/271\n",
      "Processed frame 216/271\n",
      "Processed frame 217/271\n",
      "Processed frame 218/271\n",
      "Processed frame 219/271\n",
      "Processed frame 220/271\n",
      "Processed frame 221/271\n",
      "Processed frame 222/271\n",
      "Processed frame 223/271\n",
      "Processed frame 224/271\n",
      "Processed frame 225/271\n",
      "Processed frame 226/271\n",
      "Processed frame 227/271\n",
      "Processed frame 228/271\n",
      "Processed frame 229/271\n",
      "Processed frame 230/271\n",
      "Processed frame 231/271\n",
      "Processed frame 232/271\n",
      "Processed frame 233/271\n",
      "Processed frame 234/271\n",
      "Processed frame 235/271\n",
      "Processed frame 236/271\n",
      "Processed frame 237/271\n",
      "Processed frame 238/271\n",
      "Processed frame 239/271\n",
      "Processed frame 240/271\n",
      "Processed frame 241/271\n",
      "Processed frame 242/271\n",
      "Processed frame 243/271\n",
      "Processed frame 244/271\n",
      "Processed frame 245/271\n",
      "Processed frame 246/271\n",
      "Processed frame 247/271\n",
      "Processed frame 248/271\n",
      "Processed frame 249/271\n",
      "Processed frame 250/271\n",
      "Processed frame 251/271\n",
      "Processed frame 252/271\n",
      "Processed frame 253/271\n",
      "Processed frame 254/271\n",
      "Processed frame 255/271\n",
      "Processed frame 256/271\n",
      "Processed frame 257/271\n",
      "Processed frame 258/271\n",
      "Processed frame 259/271\n",
      "Processed frame 260/271\n",
      "Processed frame 261/271\n",
      "Processed frame 262/271\n",
      "Processed frame 263/271\n",
      "Processed frame 264/271\n",
      "Processed frame 265/271\n",
      "Processed frame 266/271\n",
      "Processed frame 267/271\n",
      "Processed frame 268/271\n",
      "Processed frame 269/271\n",
      "Processed frame 270/271\n",
      "Processed frame 271/271\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "md1_path = r'D:\\Jatin\\College stuff\\Major Project\\Crowd\\crowd-density-aspp\\Models\\aspp\\aspp_1_800.pt'\n",
    "md1 = torch.load(md1_path, map_location=device).to(device)\n",
    "md1.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Read video file\n",
    "video_path = r'D:\\Jatin\\College stuff\\Major Project\\Crowd\\crowd-density-aspp\\263C044_060_c.mov'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Set resize parameters\n",
    "resize_width = 640  # Change this according to your preference\n",
    "resize_height = 480  # Change this according to your preference\n",
    "\n",
    "# Create window to display analyzed frames\n",
    "cv2.namedWindow('Analyzed Frame', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Analyzed Frame', resize_width, resize_height)\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "\n",
    "    # Convert frame to PIL Image\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Apply transformation\n",
    "    input_image = transform(frame_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output_density_map = md1(input_image)\n",
    "\n",
    "    # Convert the output density map tensor to numpy array\n",
    "    density_map_numpy = output_density_map.squeeze().cpu().numpy()\n",
    "\n",
    "    # Resize density map to match frame size\n",
    "    density_map_resized = cv2.resize(density_map_numpy, (resize_width, resize_height))\n",
    "\n",
    "    # Normalize density map for visualization\n",
    "    density_map_resized = cv2.normalize(density_map_resized, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    # Overlay density map on frame\n",
    "    alpha = 0.5\n",
    "    overlay = cv2.applyColorMap(density_map_resized, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "    # Display the analyzed frame\n",
    "    cv2.imshow('Analyzed Frame', overlay)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Display progress\n",
    "    frame_count += 1\n",
    "    print(f'Processed frame {frame_count}/{total_frames}')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mJatin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCollege stuff\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMajor Project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCrowd\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcrowd-density-aspp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maspp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maspp_2_800.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Define the transformation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jatin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jatin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jatin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model\n",
    "md1_path = r'D:\\Jatin\\College stuff\\Major Project\\Crowd\\crowd-density-aspp\\Models\\aspp\\aspp_2_800.pt'\n",
    "md1 = torch.load(md1_path, map_location=device).to(device)\n",
    "md1.eval()\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Read video file\n",
    "video_path = r'D:\\Jatin\\College stuff\\Major Project\\Data\\263C044_060_c.mov'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Set resize parameters\n",
    "resize_width = 640  # Change this according to your preference\n",
    "resize_height = 480  # Change this according to your preference\n",
    "\n",
    "# Function to get bounding boxes\n",
    "def get_bounding_boxes(density_map, num_boxes=5):\n",
    "    # Threshold density map\n",
    "    thresholded_map = cv2.threshold(density_map, 50, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresholded_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours by area in descending order\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:num_boxes]\n",
    "\n",
    "    # Get bounding boxes\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "\n",
    "    return bounding_boxes\n",
    "\n",
    "# Create window to display analyzed frames\n",
    "cv2.namedWindow('Analyzed Frame', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Analyzed Frame', resize_width, resize_height)\n",
    "\n",
    "# Define the codec and create VideoWriter object for MP4\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec for MP4 format\n",
    "output_video_path = r'D:\\Jatin\\College stuff\\Major Project\\Data\\output_video.mp4'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (resize_width, resize_height))\n",
    "\n",
    "\n",
    "# Process each frame\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize frame\n",
    "    frame = cv2.resize(frame, (resize_width, resize_height))\n",
    "\n",
    "    # Convert frame to PIL Image\n",
    "    frame_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Apply transformation\n",
    "    input_image = transform(frame_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output_density_map = md1(input_image)\n",
    "\n",
    "    # Convert the output density map tensor to numpy array\n",
    "    density_map_numpy = output_density_map.squeeze().cpu().numpy()\n",
    "\n",
    "    # Resize density map to match frame size\n",
    "    density_map_resized = cv2.resize(density_map_numpy, (resize_width, resize_height))\n",
    "\n",
    "    # Normalize density map for visualization\n",
    "    density_map_resized = cv2.normalize(density_map_resized, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "\n",
    "    # Overlay density map on frame\n",
    "    alpha = 0.5\n",
    "    overlay = cv2.applyColorMap(density_map_resized, cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "    # Get bounding boxes for high density areas\n",
    "    bounding_boxes = get_bounding_boxes(density_map_resized)\n",
    "\n",
    "    # Draw bounding boxes on overlay\n",
    "    for x, y, w, h in bounding_boxes:\n",
    "        cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Write frame to output video\n",
    "    out.write(overlay)\n",
    "\n",
    "    # Display the analyzed frame\n",
    "    cv2.imshow('Analyzed Frame', overlay)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Display progress\n",
    "    frame_count += 1\n",
    "    print(f'Processed frame {frame_count}/{total_frames}')\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()  # Release VideoWriter\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
